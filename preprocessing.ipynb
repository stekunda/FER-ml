{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not do run all, run the cells one at a time\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torch.utils.data import ConcatDataset\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 48\n",
    "IMG_WIDTH = 48\n",
    "\n",
    "# Path to the training data\n",
    "TRAIN_DATA_PATH = os.path.join(os.getcwd(), '../FER with DL/data', 'train')\n",
    "\n",
    "# Path to the test data\n",
    "TEST_DATA_PATH = os.path.join(os.getcwd(), '../FER with DL/data', 'test')\n",
    "\n",
    "# Define your transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the datasets\n",
    "train_dataset = ImageFolder(TRAIN_DATA_PATH, transform=transform)\n",
    "test_dataset = ImageFolder(TEST_DATA_PATH, transform=transform)\n",
    "\n",
    "# Create the dataloader for validation set only, train data still needs to be augmented\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes roughly 4 minutes to run this cell\n",
    "\n",
    "# Oversampling the disgust samples since we don't have many samples\n",
    "\n",
    "# Define additional transformations for data augmentation\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create a new dataset with only the \"disgust\" images\n",
    "disgust_dataset = [img for img in train_dataset if img[1]\n",
    "                   == train_dataset.class_to_idx['disgust']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Tensor to PIL Image\n",
    "to_pil = ToPILImage()\n",
    "\n",
    "# Apply data augmentation to the \"disgust\" images\n",
    "augmented_disgust_dataset = [(augment_transform(to_pil(img[0])), img[1])\n",
    "                             for _ in range(3) for img in disgust_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(augmented_disgust_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the original dataset with the augmented \"disgust\" images\n",
    "train_dataset = ConcatDataset([train_dataset, augmented_disgust_dataset])\n",
    "\n",
    "# Update the train DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Print details about the train and test datasets\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Binary patterns (LBP) Implementation - 35% Accuracy\n",
    "def extract_features_lbp(data_loader):\n",
    "    # Parameters for LBP\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    # Initialize an empty list to store the feature vectors\n",
    "    features = []\n",
    "    labels = []\n",
    "    # Process each batch of images\n",
    "    for images, batch_labels in data_loader:\n",
    "        # Process each image in the batch\n",
    "        for i in range(len(images)):\n",
    "            image = images[i].numpy().squeeze()\n",
    "            # Apply LBP\n",
    "            lbp = local_binary_pattern(image, n_points, radius)\n",
    "            # Flatten the LBP image and add it to the list of feature vectors\n",
    "            features.append(lbp.ravel())\n",
    "            # Add the corresponding label to the labels list\n",
    "            labels.append(batch_labels[i].item())\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenfaces Implmentation - 14% Accuracy\n",
    "def extract_features_eigenfaces(data_loader):\n",
    "    # Initialize an empty list to store the images and labels\n",
    "    images = []\n",
    "    labels = []\n",
    "    # Process each batch of images\n",
    "    for batch_images, batch_labels in data_loader:\n",
    "        # Process each image in the batch\n",
    "        for i in range(len(batch_images)):\n",
    "            image = batch_images[i].numpy().squeeze()\n",
    "            # Flatten the image and add it to the list of images\n",
    "            images.append(image.ravel())\n",
    "            # Add the corresponding label to the labels list\n",
    "            labels.append(batch_labels[i].item())\n",
    "    # Convert the lists to numpy arrays\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    # Apply PCA to the images\n",
    "    pca = PCA(n_components=150)\n",
    "    features = pca.fit_transform(images)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fisherfaces Implementation - 12.45% Accuracy\n",
    "def extract_features_fisherfaces(data_loader):\n",
    "    # Initialize an empty list to store the images and labels\n",
    "    images = []\n",
    "    labels = []\n",
    "    # Process each batch of images\n",
    "    for batch_images, batch_labels in data_loader:\n",
    "        # Process each image in the batch\n",
    "        for i in range(len(batch_images)):\n",
    "            image = batch_images[i].numpy().squeeze()\n",
    "            # Flatten the image and add it to the list of images\n",
    "            images.append(image.ravel())\n",
    "            # Add the corresponding label to the labels list\n",
    "            labels.append(batch_labels[i].item())\n",
    "    # Convert the lists to numpy arrays\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    # Apply PCA to the images\n",
    "    pca = PCA(n_components=150)\n",
    "    pca_result = pca.fit_transform(images)\n",
    "    # Apply LDA to the PCA result\n",
    "    lda = LDA(n_components=None)  # Automatically set n_components to min(n_features, n_classes - 1)\n",
    "    features = lda.fit_transform(pca_result, labels)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes roughly 4 minutes to run this cell\n",
    "X_train, y_train = extract_features_eigenfaces(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes roughly 4 minutes to run this cell\n",
    "X_test, y_test = extract_features_eigenfaces(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Number of feature vectors: {len(X_train)}\")\n",
    "print(f\"Shape of a feature vector: {X_train[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(X_train, y_train, X_test, y_test):\n",
    "    # Create a SVM (Support Vector Machine) classifier\n",
    "    clf = svm.SVC()\n",
    "\n",
    "    # Train the classifier with the training data and labels\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels of the test data\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the classifier\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_evaluate(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset.class_to_idx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DA515",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
